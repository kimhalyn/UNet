{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing_unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFBBuu+N7fBHJy/qy/ArhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimhalyn/UNet/blob/main/testing_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9qSwf1YEmyc"
      },
      "source": [
        "# UNet 구조를 구현하는 코드 작성\n",
        "# 필요한 라이브러리 추가하기\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-3VN8DTXZqu",
        "outputId": "2665387c-5bca-4771-e4eb-bef7f1b38378"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0usHFoccFtAI"
      },
      "source": [
        "## 트레이닝에 필요한 하이퍼파라미터 설정(수정)\n",
        "lr = 1e-3\n",
        "batch_size = 4\n",
        "num_epoch = 100\n",
        "\n",
        "data_dir = './drive/MyDrive/training_unet/datasets' # 데이터가 저장된 디렉토리\n",
        "ckpt_dir = './drive/MyDrive/training_unet/checkpoint' # 트레이닝 된 네트워크가 저장될 체크포인트 디렉토리\n",
        "log_dir = './drive/MyDrive/training_unet/log' # 텐서보드 로그파일이 저장될 로그 디렉토리\n",
        "result_dir = './drive/MyDrive/training_unet/results' # 출력된 output을 numpy 데이터셋으로 저장\n",
        "\n",
        "# 폴더 생성\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir, 'numpy'))\n",
        "\n",
        "# GPU 머신에서 동작할지, CPU 머신에서 동작할지 결정해주는 device flag 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te-WUuqKPwaj"
      },
      "source": [
        "# 네트워크 구축하기\n",
        "## UNet 레이어 생성하기\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "\n",
        "          # convolution layer에 대한 정의\n",
        "            layers = []\n",
        "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                 bias=bias)]\n",
        "          # batch normalization layer에 대한 정의\n",
        "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "\n",
        "          # ReLU layer에 대한 정의  \n",
        "            layers += [nn.ReLU()]\n",
        "\n",
        "            cbr = nn.Sequential(*layers)\n",
        "\n",
        "            return cbr\n",
        "\n",
        "        # Contracting path = encoder part\n",
        "        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n",
        "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
        "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
        "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
        "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
        "\n",
        "        # Expansive path = decoder part\n",
        "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
        "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
        "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
        "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
        "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "#UNet 레이어 연결하기\n",
        "    def forward(self, x): # x: input image\n",
        "        \n",
        "        # encoder part 연결\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2) # input이 크기가 줄어든 output으로 출력\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        #decoder part 연결\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1) # dim -> 0:batch, 1:channel 2:height(=y방향), 3:width(x방향) \n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1) \n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        x = self.fc(dec1_1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR-Ncd1c4kk3"
      },
      "source": [
        "# Data loader & transform 구현\n",
        "## Data loader 구현\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        lst_data = os.listdir(self.data_dir) # data_dir에 있는 모든 파일 리스트 가져오기\n",
        "\n",
        "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
        "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
        "\n",
        "        lst_label.sort()\n",
        "        lst_input.sort()\n",
        "\n",
        "        self.lst_label = lst_label\n",
        "        self.lst_input = lst_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lst_label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
        "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
        "\n",
        "        # 0~1사이로 정규화\n",
        "        label = label/255.0\n",
        "        input = input/255.0\n",
        "\n",
        "        if label.ndim == 2:\n",
        "            label = label[:, :, np.newaxis]\n",
        "        if input.ndim == 2:\n",
        "            input = input[:, :, np.newaxis]\n",
        "\n",
        "        data = {'input': input, 'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlX8mVrBqbRC"
      },
      "source": [
        "# transform 구현\n",
        "class ToTensor(object): # ToTensor(): numpy -> tensor\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        label = label.transpose((2, 0, 1)).astype(np.float32)  # Image의 tensor 차원 = (CH, Y, X)\n",
        "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
        "\n",
        "        return data\n",
        "\n",
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        input = (input - self.mean) / self.std\n",
        "\n",
        "        data = {'label': label, 'input': input}\n",
        "\n",
        "        return data\n",
        "\n",
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        label, input = data['label'], data['input']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.fliplr(label)\n",
        "            input = np.fliplr(input)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            label = np.flipud(label)\n",
        "            input = np.flipud(input)\n",
        "\n",
        "        data = {'label': label, 'input': input}\n",
        "\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHsBNQ-5rRMe",
        "outputId": "68dfdff3-1f31-4b12-bb5f-d988dbdec928"
      },
      "source": [
        "# 네트워크 학습하기(수정)\n",
        "transform = transforms.Compose([Normalization(mean=0.5, std=0.5), ToTensor()])\n",
        "\n",
        "dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vioCwYkB1vxk"
      },
      "source": [
        "# 네트워크 생성하기\n",
        "net = UNet().to(device) # 네트워크를 불러오고 학습 도메인이 CPU인지 GPU인지 명시해주기 위해 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOH2PmBu3hmX"
      },
      "source": [
        "## 손실함수 정의하기\n",
        "fn_loss = nn.BCEWithLogitsLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3IObxKM3mpr"
      },
      "source": [
        "## Optimizer 설정하기\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2E4W2-M3pc6"
      },
      "source": [
        "## 그밖에 부수적인 variables 설정하기(수정)\n",
        "num_data_test = len(dataset_test)\n",
        "\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eArG5y-m3sLs"
      },
      "source": [
        "## 그밖에 부수적인 functions 설정하기\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxJWNVLp3x4K"
      },
      "source": [
        "## Tensorboard를 사용하기 위한 SummaryWriter 설정(수정)\n",
        "#사용하지 않으므로 삭제\n",
        "#writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "#writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVrpUNmE8tkW"
      },
      "source": [
        "## 네트워크 저장하기\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZfBO69J8ukd"
      },
      "source": [
        "## 네트워크 불러오기\n",
        "def load(ckpt_dir, net, optim):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return net, optim, epoch\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "    net.load_state_dict(dict_model['net'])\n",
        "    optim.load_state_dict(dict_model['optim'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return net, optim, epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8RQ4FRaHagq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fe4229-5757-4dd6-d80a-2a9d58a97a0d"
      },
      "source": [
        "## 네트워크 학습시키기(수정)\n",
        "st_epoch = 0\n",
        "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "# epoch이 존재하지 않으므로 삭제\n",
        "\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(loader_test, 1):\n",
        "        # forward pass\n",
        "        label = data['label'].to(device)\n",
        "        input = data['input'].to(device)\n",
        "\n",
        "        output = net(input)\n",
        "\n",
        "        # 손실함수 계산하기\n",
        "        loss = fn_loss(output, label)\n",
        "\n",
        "        loss_arr += [loss.item()]\n",
        "\n",
        "        print(\"TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
        "              (batch, num_batch_test, np.mean(loss_arr)))\n",
        "\n",
        "        # Tensorboard 저장하기\n",
        "        label = fn_tonumpy(label)\n",
        "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "        output = fn_tonumpy(fn_class(output))\n",
        "\n",
        "#writer 존재하지 않으므로 삭제\n",
        "\n",
        "\n",
        "        for j in range(label.shape[0]):\n",
        "            id = num_batch_test * (batch - 1) + j\n",
        "\n",
        "            # png 타입으로 저장\n",
        "            plt.imsave(os.path.join(result_dir, 'png', 'label_%04d.png' % id), label[j].squeeze(), cmap='gray')\n",
        "            plt.imsave(os.path.join(result_dir, 'png', 'input_%04d.png' % id), input[j].squeeze(), cmap='gray')\n",
        "            plt.imsave(os.path.join(result_dir, 'png', 'output_%04d.png' % id), output[j].squeeze(), cmap='gray')\n",
        "\n",
        "            # numpy 타입으로 저장\n",
        "            np.save(os.path.join(result_dir, 'numpy', 'label_%04d.npy' % id), label[j].squeeze())\n",
        "            np.save(os.path.join(result_dir, 'numpy', 'input_%04d.npy' % id), input[j].squeeze())\n",
        "            np.save(os.path.join(result_dir, 'numpy', 'output_%04d.npy' % id), output[j].squeeze())\n",
        "\n",
        "print(\"AVERAGE TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
        "      (batch, num_batch_test, np.mean(loss_arr))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST: BATCH 0001 / 0001 | LOSS 0.1870\n",
            "AVERAGE TEST: BATCH 0001 / 0001 | LOSS 0.1870\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}